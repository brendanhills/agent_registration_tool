{
    "project_id": "uk-bh-experiments-argolis",
    "location": "us-central1", 
    "re_resource_name": "projects/376231489344/locations/us-central1/reasoningEngines/374810319769305088",
    "re_resource_id": "",
    "re_display_name": "llm_auditor",
    "app_id": "ngrid-responsibility-demo_1749891625901",
    "agent_id": "2437637000651170151",
    "ars_display_name": "LLM Auditor",
    "description": "This agent functions as an automated fact-checking layer specifically designed to evaluate and enhance the factual grounding of responses generated by Large Language Models (LLMs). Its primary role is to bolster the reliability of LLM outputs by systematically analyzing them against real-world information; it achieves this by identifying verifiable claims within the text, utilizing web search and its internal knowledge to determine their accuracy, producing a detailed report on its findings, and optionally rewriting the original response to correct any discovered inaccuracies.",
    "tool_description": "This agent functions as an automated fact-checking layer specifically designed to evaluate and enhance the factual grounding of responses generated by Large Language Models (LLMs). Its primary role is to bolster the reliability of LLM outputs by systematically analyzing them against real-world information; it achieves this by identifying verifiable claims within the text, utilizing web search and its internal knowledge to determine their accuracy, producing a detailed report on its findings, and optionally rewriting the original response to correct any discovered inaccuracies.",
    "adk_deployment_id": "374810319769305088",
    "auth_id": "",
    "icon_uri": "",
    "api_location": "global",
    "re_location": "us-central1"
}


